{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0a7e46-1fd8-4b80-887c-376a64bb8571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "\n",
    "# Define the time intervals and corresponding MET values\n",
    "intervals = {\n",
    "    'walk_1': {'times': ('15:36:00', '15:41:00'), 'MET': 3.0},\n",
    "    'walk_2': {'times': ('15:43:00', '15:47:00'), 'MET': 3.5},\n",
    "    'walk_3': {'times': ('15:49:00', '15:53:00'), 'MET': 4.0},\n",
    "    'walk_4': {'times': ('15:54:30', '15:56:00'), 'MET': 4.5},\n",
    "    'run_1': {'times': ('15:59:00', '16:03:00'), 'MET': 8.0},\n",
    "    'run_2': {'times': ('16:06:30', '16:09:30'), 'MET': 10.0}\n",
    "}\n",
    "\n",
    "\n",
    "# Convert time strings to datetime objects\n",
    "for key in intervals:\n",
    "    start_time_str, end_time_str = intervals[key]['times']\n",
    "    intervals[key]['times'] = (\n",
    "        datetime.strptime(start_time_str, '%H:%M:%S'),\n",
    "        datetime.strptime(end_time_str, '%H:%M:%S')\n",
    "    )\n",
    "\n",
    "# Function to extract features from acceleration data\n",
    "def detrended_fluctuation_analysis(signal):\n",
    "    n = len(signal)\n",
    "    lags = np.arange(2, n // 2)\n",
    "    tau = np.zeros(lags.size)\n",
    "    \n",
    "    for i, lag in enumerate(lags):\n",
    "        segments = n // lag\n",
    "        reshaped_signal = signal[:segments * lag].reshape(segments, lag)\n",
    "        local_trend = np.polyval(np.polyfit(np.arange(lag), reshaped_signal.T, 1), np.arange(lag))\n",
    "        detrended = reshaped_signal - local_trend.T\n",
    "        tau[i] = np.sqrt(np.mean(detrended ** 2))\n",
    "    \n",
    "    alpha = np.polyfit(np.log(lags), np.log(tau), 1)[0]\n",
    "    return alpha\n",
    "\n",
    "def extract_features(acc_data):\n",
    "    mean = np.mean(acc_data, axis=0)\n",
    "    std_dev = np.std(acc_data, axis=0)\n",
    "    var = np.var(acc_data, axis=0)\n",
    "    skewness = skew(acc_data, axis=0)\n",
    "    kurt = kurtosis(acc_data, axis=0)\n",
    "    quantiles = np.percentile(acc_data, np.arange(10, 110, 10), axis=0)\n",
    "    alpha = detrended_fluctuation_analysis(acc_data[:, 3])  # Using magnitude for DFA\n",
    "    return [mean, std_dev, var, skewness, kurt, quantiles, alpha]\n",
    "    # return [mean, std_dev, var, skewness, kurt]\n",
    "\n",
    "# Define high-pass filter parameters\n",
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def highpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_highpass(cutoff, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Read the data from the CSV file and process it\n",
    "acc_data = []\n",
    "with open(\"/Users/sh/Documents/FdS/Master 2 IDS/S1/Sant√© AP/Data/data_motet.csv\", 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        try:\n",
    "            if len(row) < 4:\n",
    "                raise ValueError(\"Row has missing values\")\n",
    "            timestamp = datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S.%f')\n",
    "            x = float(row[1])\n",
    "            y = float(row[2])\n",
    "            z = float(row[3])\n",
    "            acc_data.append([timestamp, x, y, z])\n",
    "        except (ValueError, IndexError) as e:\n",
    "            print(f\"Skipping row due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "# Sampling frequency and cutoff frequency\n",
    "fs = 50.0    # Sampling frequency in Hz\n",
    "cutoff = 0.3  # Cutoff frequency in Hz\n",
    "\n",
    "# Process the data for each interval\n",
    "stats_list = []\n",
    "for label, info in intervals.items():\n",
    "    start_time = info['times'][0].time()\n",
    "    end_time = info['times'][1].time()\n",
    "    \n",
    "    interval_data = [row for row in acc_data if start_time <= row[0].time() <= end_time]\n",
    "    if not interval_data:\n",
    "        print(f\"No data found for interval: {label}\")\n",
    "        continue\n",
    "    \n",
    "    interval_data = np.array(interval_data, dtype=object)\n",
    "    interval_data = interval_data[:, 1:].astype(float)\n",
    "    x_accel = interval_data[:, 0]\n",
    "    y_accel = interval_data[:, 1]\n",
    "    z_accel = interval_data[:, 2]\n",
    "    \n",
    "    # Calculate magnitude\n",
    "    accel_magnitude = np.sqrt(np.square(x_accel) + np.square(y_accel) + np.square(z_accel))\n",
    "    \n",
    "    # Apply high-pass filter for gravity compensation\n",
    "    x_accel_compensated = highpass_filter(x_accel, cutoff, fs)\n",
    "    y_accel_compensated = highpass_filter(y_accel, cutoff, fs)\n",
    "    z_accel_compensated = highpass_filter(z_accel, cutoff, fs)\n",
    "    accel_magnitude_compensated = highpass_filter(accel_magnitude, cutoff, fs)\n",
    "    \n",
    "    # Extract features\n",
    "    features = extract_features(np.column_stack((x_accel_compensated, y_accel_compensated, z_accel_compensated,accel_magnitude_compensated)))\n",
    "    \n",
    "    stats_list.append({\n",
    "        'Interval': label,\n",
    "        'Mean': features[0],\n",
    "        'Standard Deviation': features[1],\n",
    "        'Variance': features[2],\n",
    "        'Skewness': features[3],\n",
    "        'Kurtosis': features[4],\n",
    "        'Quantiles': features[5],\n",
    "        'DFA': features[6],\n",
    "        'MET': info['MET']\n",
    "    })\n",
    "\n",
    "# Prepare the dataset for regression\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for stats in stats_list:\n",
    "    features = [\n",
    "        stats['Mean'],\n",
    "        stats['Standard Deviation'],\n",
    "        stats['Variance'],\n",
    "        stats['Skewness'],\n",
    "        stats['Kurtosis']\n",
    "    ]\n",
    "    X.append(np.concatenate(features).ravel())\n",
    "    y.append(stats['MET'])\n",
    "\n",
    "# Convert lists to numpy arrays for modeling\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split the data into training and validation sets (e.g., 80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation data\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Predict MET values based on extracted features\n",
    "predicted_MET = model.predict(X)\n",
    "\n",
    "# Display actual vs predicted MET values\n",
    "print(\"Interval\\tActual MET\\tPredicted MET\")\n",
    "for idx, stats in enumerate(stats_list):\n",
    "    print(f\"{stats['Interval']}\\t{stats['MET']}\\t{predicted_MET[idx]}\")\n",
    "plt.show()\n",
    "\n",
    "# Function to plot intervals\n",
    "def plot_intervals(acc_data, intervals):\n",
    "    fig, axs = plt.subplots(len(intervals), 1, figsize=(10, 15))\n",
    "    \n",
    "    for i, (label, info) in enumerate(intervals.items()):\n",
    "        start_time = info['times'][0].time()\n",
    "        end_time = info['times'][1].time()\n",
    "        \n",
    "        interval_data = [row for row in acc_data if start_time <= row[0].time() <= end_time]\n",
    "        if not interval_data:\n",
    "            print(f\"No data found for interval: {label}\")\n",
    "            continue\n",
    "        \n",
    "        interval_data = np.array(interval_data, dtype=object)\n",
    "        interval_data = interval_data[:, 1:].astype(float)\n",
    "        x_accel = interval_data[:, 0]\n",
    "        y_accel = interval_data[:, 1]\n",
    "        z_accel = interval_data[:, 2]\n",
    "        \n",
    "        # Calculate magnitude\n",
    "        accel_magnitude = np.sqrt(np.square(x_accel) + np.square(y_accel) + np.square(z_accel))\n",
    "        \n",
    "        axs[i].plot(accel_magnitude)\n",
    "        axs[i].set_title(f'Interval {label}')\n",
    "        axs[i].set_xlabel('Time')\n",
    "        axs[i].set_ylabel('Magnitude')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot actual vs predicted MET values\n",
    "plt.plot(y, label='Actual MET')\n",
    "plt.plot(predicted_MET, label='Predicted MET')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Call the plot_intervals function\n",
    "plot_intervals(acc_data, intervals)\n",
    "\n",
    "# Plot the features for each interval\n",
    "features = ['Mean', 'Standard Deviation', 'Variance', 'Skewness', 'Kurtosis']\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for stats in stats_list:\n",
    "        plt.plot(stats[feature], label=stats['Interval'])\n",
    "    plt.title(f'{feature} for each interval')\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel(feature)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Train a Ridge regression model\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation data using Ridge regression\n",
    "y_pred_ridge = ridge_model.predict(X_val)\n",
    "\n",
    "# Evaluate the Ridge model's performance\n",
    "mse_ridge = mean_squared_error(y_val, y_pred_ridge)\n",
    "print(\"Ridge Regression Mean Squared Error:\", mse_ridge)\n",
    "\n",
    "# Train a Lasso regression model\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation data using Lasso regression\n",
    "y_pred_lasso = lasso_model.predict(X_val)\n",
    "\n",
    "# Evaluate the Lasso model's performance\n",
    "mse_lasso = mean_squared_error(y_val, y_pred_lasso)\n",
    "print(\"Lasso Regression Mean Squared Error:\", mse_lasso)\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_model = DecisionTreeRegressor(max_depth=5) \n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "forest_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gbr_model.fit(X_train, y_train)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_model = MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_val_pred = forest_model.predict(X_val)\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# CNN\n",
    "window_size = 50  \n",
    "stride = 25     \n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for stats in stats_list:\n",
    "    interval_data = [row for row in acc_data if stats['Interval'] == row[0]]\n",
    "    data_length = len(interval_data)\n",
    "\n",
    "    for start in range(0, data_length - window_size + 1, stride):\n",
    "        window = interval_data[start:start + window_size]\n",
    "        features = np.array(window).T  # Transformer en (temps, caract√©ristiques)\n",
    "        X.append(features)\n",
    "        y.append(stats['MET'])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train = X_train / np.max(X_train)\n",
    "X_val = X_val / np.max(X_val)\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(window_size, X_train.shape[2])),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "y_pred_cnn = model.predict(X_val)\n",
    "mse_cnn = mean_squared_error(y_val, y_pred_cnn)\n",
    "print(\"CNN Mean Squared Error:\", mse_cnn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
